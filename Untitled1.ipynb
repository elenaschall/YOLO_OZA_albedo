{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b60674-c893-450d-8efd-3566694cd26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn.functional as F_general\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f11609f-ecdf-4901-af18-6c03f3ba5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714d0b09-6ecc-4f1b-a87b-d3d843834ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset:\n",
    "    def __init__(self, config, path_to_dataset):\n",
    "        # Spectrogram settings\n",
    "        self.duration = config['duration']\n",
    "        self.overlap = config['overlap']  # overlap of the chunks in %\n",
    "        self.desired_fs = config['desired_fs']\n",
    "        self.channel = config['channel']\n",
    "        self.log = config['log']\n",
    "        self.color = config['color']\n",
    "\n",
    "        self.nfft = config['nfft']\n",
    "        self.win_len = config['win_len']\n",
    "        self.hop_len = config['hop_len']\n",
    "        self.win_overlap = self.win_len - self.hop_len\n",
    "\n",
    "        # Get the color map by name:\n",
    "        #self.cmap = plt.get_cmap(config['cmap'])\n",
    "\n",
    "        # Path to dataset\n",
    "        self.path_to_dataset = pathlib.Path(path_to_dataset)\n",
    "        self.wavs_folder = self.path_to_dataset.joinpath('audio')\n",
    "        self.annotations_folder = self.path_to_dataset.joinpath('annotations')\n",
    "        self.images_folder = pathlib.Path('/albedo/work/projects/p_OZA_AI/train_test/images')\n",
    "        self.labels_folder = pathlib.Path('/albedo/work/projects/p_OZA_AI/train_test/labels')\n",
    "        if not self.images_folder.exists():\n",
    "            os.mkdir(self.images_folder)\n",
    "            os.mkdir(self.labels_folder)\n",
    "            os.mkdir(self.labels_folder.joinpath('backgrounds'))\n",
    "\n",
    "        self.F_MIN = 0\n",
    "        self.blocksize = int(self.duration * self.desired_fs)\n",
    "        self.config = config\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self.config.keys():\n",
    "            self.config[key] = value\n",
    "        self.__dict__[key] = value\n",
    "\n",
    "    def save_config(self, config_path):\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f)\n",
    "\n",
    "    def create_train_dataset(self, class_encoding):\n",
    "        indices_per_deployment = self.convert_challenge_annotations_to_yolo(class_encoding=class_encoding)\n",
    "        selected_samples = self.select_background_labels(indices_per_deployment)\n",
    "        self.create_spectrograms(selected_samples=selected_samples)\n",
    "\n",
    "    def create_test_dataset(self, class_encoding):\n",
    "        indices_per_deployment = self.convert_challenge_annotations_to_yolo(class_encoding=class_encoding)\n",
    "        selected_samples = self.select_all_background_labels(indices_per_deployment)\n",
    "        self.create_spectrograms(selected_samples=selected_samples)\n",
    "\n",
    "    def select_all_background_labels(self, indices_per_deployment):\n",
    "        for dataset, indices_dataset in indices_per_deployment.items():\n",
    "            background_indices = indices_dataset['background']\n",
    "            indices_per_deployment[dataset]['selected_background'] = background_indices\n",
    "\n",
    "            for selection in background_indices:\n",
    "                shutil.move(self.labels_folder.joinpath('backgrounds', selection + '.txt'),\n",
    "                            self.labels_folder.joinpath(selection + '.txt'))\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def select_background_labels(self, indices_per_deployment):\n",
    "        for dataset, indices_dataset in indices_per_deployment.items():\n",
    "            label_indices = indices_dataset['labels']\n",
    "            background_indices = indices_dataset['background']\n",
    "            n_labels = min(len(label_indices), len(background_indices))\n",
    "            print(f'There are {n_labels} labels in the dataset {dataset}')\n",
    "\n",
    "            selected_background = random.sample(background_indices, n_labels)\n",
    "            indices_per_deployment[dataset]['selected_background'] = selected_background\n",
    "\n",
    "            for selection in selected_background:\n",
    "                shutil.move(self.labels_folder.joinpath('backgrounds', selection + '.txt'),\n",
    "                            self.labels_folder.joinpath(selection + '.txt'))\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def create_spectrograms(self, selected_samples, overwrite=True):\n",
    "        # First, create all the images\n",
    "        print('Creating spectrograms...')\n",
    "        for dataset, indices_dataset in selected_samples.items():\n",
    "            selected_indices = indices_dataset['selected_background'] + indices_dataset['labels']\n",
    "            for sample_i in tqdm(selected_indices):\n",
    "                img_path = self.images_folder.joinpath(sample_i + '.png')\n",
    "                wav_name = '_'.join(sample_i.split('_')[1:3])\n",
    "                wav_path = self.wavs_folder.joinpath(dataset, wav_name + '.wav')\n",
    "                i = float(sample_i.split('_')[-1])\n",
    "                if overwrite or (not img_path.exists()):\n",
    "                    start_chunk = int(i * self.blocksize)\n",
    "                    chunk, fs = torchaudio.load(wav_path, normalize=True, frame_offset=start_chunk,\n",
    "                                                num_frames=self.blocksize)\n",
    "                    chunk = chunk[0, :]\n",
    "\n",
    "                    if len(chunk) < self.blocksize:\n",
    "                        chunk = F_general.pad(chunk, (0, self.blocksize - len(chunk)))\n",
    "                    img, f = self.create_chunk_spectrogram(chunk)\n",
    "\n",
    "                    if self.log:\n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.pcolormesh(img[:, :, ::-1])\n",
    "                        ax.set_yscale('symlog')\n",
    "                        plt.axis('off')\n",
    "                        plt.ylim(bottom=3)\n",
    "                        plt.savefig(img_path, bbox_inches='tight', pad_inches=0)\n",
    "                    else:\n",
    "                        Image.fromarray(np.flipud(img)).save(img_path)\n",
    "                    plt.close()\n",
    "                i += self.overlap\n",
    "\n",
    "    def create_chunk_spectrogram(self, chunk):\n",
    "        sos = scipy.signal.iirfilter(20, [5, 124], rp=None, rs=None, btype='band',\n",
    "                                     analog=False, ftype='butter', output='sos',\n",
    "                                     fs=self.desired_fs)\n",
    "        chunk = scipy.signal.sosfilt(sos, chunk)\n",
    "        f, t, sxx = scipy.signal.spectrogram(chunk, fs=self.desired_fs, window=('hann'),\n",
    "                                             nperseg=self.win_len,\n",
    "                                             noverlap=self.win_overlap, nfft=self.nfft,\n",
    "                                             detrend=False,\n",
    "                                             return_onesided=True, scaling='density', axis=-1,\n",
    "                                             mode='magnitude')\n",
    "        sxx = 1 - sxx\n",
    "        per = np.percentile(sxx.flatten(), 98)\n",
    "        sxx = (sxx - sxx.min()) / (per - sxx.min())\n",
    "        sxx[sxx > 1] = 1\n",
    "        img = np.array(sxx * 255, dtype=np.uint8)\n",
    "        return img, f\n",
    "\n",
    "    def convert_challenge_annotations_to_yolo(self, class_encoding=None):\n",
    "        \"\"\"\n",
    "        :param annotations_file:\n",
    "        :param labels_to_exclude: list\n",
    "        :param class_encoding: should be a dict with the name of the Tag as a key and an int as the value, for the\n",
    "        yolo classes\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        f_bandwidth = (self.desired_fs / 2) - self.F_MIN\n",
    "        indices_per_deployment = {}\n",
    "        for selections_path in list(self.annotations_folder.glob('*.csv')):\n",
    "            background_indices = []\n",
    "            labels_indices = []\n",
    "            selections = pd.read_csv(selections_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "            selections.loc[selections['low_frequency'] < self.F_MIN, 'low_frequency'] = self.F_MIN\n",
    "            selections['height'] = (selections['high_frequency'] - selections['low_frequency']) / f_bandwidth\n",
    "\n",
    "            # The y is from the TOP!\n",
    "            selections['y'] = 1 - (selections['high_frequency'] / f_bandwidth)\n",
    "\n",
    "            # Deal with datetime\n",
    "            selections['start_datetime_wav'] = pd.to_datetime(selections['filename'].apply(lambda y: y.split('.')[0]),\n",
    "                                                              format='%Y-%m-%dT%H-%M-%S_%f')\n",
    "            selections['start_datetime_wav'] = selections['start_datetime_wav'].dt.tz_localize('UTC')\n",
    "            selections['start_seconds'] = (selections.start_datetime - selections.start_datetime_wav).dt.total_seconds()\n",
    "            selections['end_seconds'] = (selections.end_datetime - selections.start_datetime_wav).dt.total_seconds()\n",
    "\n",
    "            pbar = tqdm(total=len(selections['filename'].unique()))\n",
    "\n",
    "            dataset_name = selections.iloc[0].dataset\n",
    "            for wav_name, wav_selections in selections.groupby('filename'):\n",
    "                wav_file_path = self.wavs_folder.joinpath(dataset_name, wav_name)\n",
    "                waveform_info = torchaudio.info(wav_file_path)\n",
    "\n",
    "                i = 0.0\n",
    "                while (i * self.duration + self.duration / 2) < (waveform_info.num_frames / waveform_info.sample_rate):\n",
    "                    start_seconds = i * self.duration\n",
    "                    end_seconds = start_seconds + self.duration\n",
    "\n",
    "                    start_mask = (wav_selections['start_seconds'] >= start_seconds) & (wav_selections[\n",
    "                                                                                           'start_seconds'] <= end_seconds)\n",
    "                    end_mask = (wav_selections['start_seconds'] >= start_seconds) & (wav_selections[\n",
    "                                                                                         'end_seconds'] <= end_seconds)\n",
    "                    chunk_selection = wav_selections.loc[start_mask | end_mask]\n",
    "                    chunk_selection = chunk_selection.assign(start_x=((chunk_selection['start_seconds'] - i * self.duration) / self.duration).clip(lower=0, upper=1).values)\n",
    "                    chunk_selection = chunk_selection.assign(end_x=((chunk_selection['end_seconds'] - i * self.duration) / self.duration).clip(lower=0, upper=1).values)\n",
    "\n",
    "                    # compute the width in pixels\n",
    "                    chunk_selection = chunk_selection.assign(width=(chunk_selection['end_x'] - chunk_selection['start_x']).values)\n",
    "\n",
    "                    # Save the chunk detections so that they are with the yolo format\n",
    "                    # <class > < x > < y > < width > < height >\n",
    "                    chunk_selection = chunk_selection.assign(x=(chunk_selection['start_x'] + chunk_selection['width'] / 2).values)\n",
    "                    chunk_selection.loc[:, 'y'] = (chunk_selection['y'] + chunk_selection['height'] / 2).values\n",
    "\n",
    "                    # if ((chunk_selection.x + chunk_selection.width/2) > 1).sum() > 0 or (chunk_selection.y > 1).sum() > 0:\n",
    "                    #     print(chunk_selection)\n",
    "                    #     print(start_seconds, end_seconds)\n",
    "                    chunk_selection = chunk_selection.replace(to_replace=class_encoding).infer_objects(copy=False)\n",
    "                    new_name = dataset_name + '_' + wav_name.replace('.wav', '_%s' % i)\n",
    "\n",
    "                    if len(chunk_selection) > 0:\n",
    "                        labels_indices.append(new_name)\n",
    "                        label_path = self.labels_folder.joinpath(new_name + '.txt')\n",
    "                    else:\n",
    "                        background_indices.append(new_name)\n",
    "                        label_path = self.labels_folder.joinpath('backgrounds', new_name + '.txt')\n",
    "\n",
    "                    chunk_selection[[\n",
    "                        'annotation',\n",
    "                        'x',\n",
    "                        'y',\n",
    "                        'width',\n",
    "                        'height']].to_csv(label_path, header=None, index=None, sep=' ', mode='w')\n",
    "                    # Add the station if the image adds it as well!\n",
    "                    i += self.overlap\n",
    "                pbar.update(1)\n",
    "            indices_per_deployment[dataset_name] = {'background': background_indices, 'labels': labels_indices}\n",
    "            pbar.close()\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def convert_yolo_detections_to_csv(self, predictions_folder, reverse_class_encoding):\n",
    "        # Convert to DataFrame\n",
    "        labels_folder = predictions_folder.joinpath('labels')\n",
    "\n",
    "        columns = ['dataset', 'filename', 'annotation', 'low_frequency', 'high_frequency',\n",
    "                   'start_datetime', 'end_datetime', 'confidence', 'offset_i']\n",
    "\n",
    "        detections_list = []\n",
    "        f_bandwidth = (self.desired_fs / 2) - self.F_MIN\n",
    "        for txt_label in tqdm(labels_folder.glob('*.txt'), total=len(list(labels_folder.glob('*.txt')))):\n",
    "            name_parts = txt_label.name.split('_')\n",
    "            wav_name = '_'.join(name_parts[1:-1]) + '.wav'\n",
    "            dataset_name = name_parts[0]\n",
    "\n",
    "            offset_i = float(name_parts[-1].split('.txt')[0])\n",
    "            detections_i = pd.read_table(txt_label, header=None, sep=' ', names=['annotation', 'x', 'y',\n",
    "                                                                                 'width', 'height', 'confidence'])\n",
    "            detections_i['filename'] = wav_name\n",
    "            detections_i['dataset'] = dataset_name\n",
    "            detections_i['offset_i'] = offset_i\n",
    "            detections_list.append(detections_i)\n",
    "\n",
    "        detections = pd.concat(detections_list, ignore_index=True)\n",
    "\n",
    "        # start and end in seconds from beginning of file\n",
    "        detections['start_seconds'] = (detections.x - detections.width / 2 + detections.offset_i) * self.duration\n",
    "        detections['end_seconds'] = detections.width * self.duration + detections['start_seconds']\n",
    "\n",
    "        # start of wav file datetime\n",
    "        detections['start_datetime_wav'] = pd.to_datetime(detections['filename'].apply(lambda y: y.split('.')[0]),\n",
    "                                                          format='%Y-%m-%dT%H-%M-%S_%f')\n",
    "        detections['start_datetime_wav'] = detections['start_datetime_wav'].dt.tz_localize('UTC')\n",
    "\n",
    "        # Compute absolute start and end time\n",
    "        detections['start_datetime'] = detections['start_datetime_wav'] + pd.to_timedelta(detections['start_seconds'],\n",
    "                                                                                          unit='s')\n",
    "        detections['end_datetime'] = detections['start_datetime_wav'] + pd.to_timedelta(detections['end_seconds'],\n",
    "                                                                                        unit='s')\n",
    "\n",
    "        detections['start_datetime'] = detections['start_datetime'].apply(lambda x: x.isoformat(timespec='microseconds'))\n",
    "        detections['end_datetime'] = detections['end_datetime'].apply(lambda x: x.isoformat(timespec='microseconds'))\n",
    "\n",
    "        # Frequency boundaries\n",
    "        detections['low_frequency'] = ((1 - (detections.y + detections.height / 2)) * f_bandwidth).clip(lower=0)\n",
    "        detections['high_frequency'] = ((1 - (detections.y - detections.height / 2)) * f_bandwidth).clip(upper=self.desired_fs/2)\n",
    "\n",
    "        # Change the annotation names\n",
    "        detections['annotation'] = detections['annotation'].replace(to_replace=reverse_class_encoding).infer_objects(copy=False)\n",
    "        detections[columns].to_csv(self.path_to_dataset.joinpath('predictions.csv'), index=False)\n",
    "\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a207c509-a04b-44c1-a0f5-bf3e567de522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 labels in the dataset ballenyislands2015\n",
      "Creating spectrograms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 49.15it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #path_to_dataset = input('Where is the dataset folder?')\n",
    "    path_to_dataset = '/isibhv/projects/p_OZA_AI/biodcase_train_vali/train_test'\n",
    "\n",
    "    #train_mode = input('Is it for the training dataset y/n?') == 'y'\n",
    "    config_path = './dataset_config.json'\n",
    "    f = open(config_path)\n",
    "    config = json.load(f)\n",
    "\n",
    "    ds = YOLODataset(config, path_to_dataset)\n",
    "    #if train_mode:\n",
    "    ds.create_train_dataset(class_encoding=config['class_encoding'])\n",
    "    #else:\n",
    "    #    ds.create_test_dataset(class_encoding=config['class_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73de9f2e-22cf-40d7-bbfa-7cf9b1f98d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import comet_ml\n",
    "except ModuleNotFoundError:\n",
    "    comet_ml = None\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf72cea-ac53-496d-8cf3-ea1ec18af218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    YAML_FILE = './custom_joined.yaml'\n",
    "    run_name = 'biodcase_baseline' # Change to the name of your run\n",
    "\n",
    "    # Check if CUDA is available\n",
    "    print('CUDA device count:')\n",
    "    print(torch.cuda.device_count())\n",
    "\n",
    "    # Read the config file\n",
    "    with open(YAML_FILE, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    if \"COMET_API_KEY\" in os.environ and comet_ml is not None:\n",
    "        experiment = comet_ml.Experiment(\n",
    "            project_name=\"biodcase\",\n",
    "        )\n",
    "\n",
    "    # Load a model\n",
    "    model = YOLO('yolo11s.pt')\n",
    "\n",
    "    # train the model\n",
    "    best_params = {\n",
    "        'iou': 0.3,\n",
    "        'imgsz': 640,\n",
    "        'hsv_s': 0,\n",
    "        'hsv_v':  0,\n",
    "        'degrees': 0,\n",
    "        'translate': 0,\n",
    "        'scale': 0,\n",
    "        'shear': 0,\n",
    "        'perspective': 0,\n",
    "        'flipud': 0,\n",
    "        'fliplr': 0,\n",
    "        'bgr': 0,\n",
    "        'mosaic': 0,\n",
    "        'mixup': 0,\n",
    "        'copy_paste': 0,\n",
    "        'erasing': 0,\n",
    "        'crop_fraction': 0,\n",
    "    }\n",
    "    model.train(epochs=20, batch=32, data=YAML_FILE,\n",
    "                project=config['path'] + '/runs/' + run_name, resume=False, **best_params)\n",
    "\n",
    "    if \"COMET_API_KEY\" in os.environ and comet_ml is not None:\n",
    "        experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79089671-d119-454a-a34c-66732c93147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device count:\n",
      "1\n",
      "New https://pypi.org/project/ultralytics/8.3.163 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.91 ðŸš€ Python-3.12.11 torch-2.7.1+cu126 CUDA:0 (NVIDIA A100 80GB PCIe, 81051MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11s.pt, data=./custom_joined.yaml, epochs=20, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/albedo/work/projects/p_OZA_AI/runs/biodcase_baseline, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.3, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0, hsv_v=0, degrees=0, translate=0, scale=0, shear=0, perspective=0, flipud=0, fliplr=0, bgr=0, mosaic=0, mixup=0, copy_paste=0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0, crop_fraction=0, cfg=None, tracker=botsort.yaml, save_dir=/albedo/work/projects/p_OZA_AI/runs/biodcase_baseline/train\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/albedo/home/eschall/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 15.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    820569  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \n",
      "YOLO11s summary: 181 layers, 9,428,953 parameters, 9,428,937 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Failed to create Comet experiment, reason: ValueError('Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.35M/5.35M [00:00<00:00, 55.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f36a9-5607-461c-a97e-426913a601a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO_OZA",
   "language": "python",
   "name": "yolo_oza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
