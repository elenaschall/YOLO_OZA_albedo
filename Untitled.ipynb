{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c559f2b8-282a-49cb-ac43-f616ef339003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate YOLO_OZA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffdd266-58be-4045-a633-eaa3518510e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn.functional as F_general\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed711ce9-731d-4cc5-beb8-496b9480b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1ec49a-2339-4136-ba91-a6b3070b6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset:\n",
    "    def __init__(self, config, path_to_dataset):\n",
    "        # Spectrogram settings\n",
    "        self.duration = config['duration']\n",
    "        self.overlap = config['overlap']  # overlap of the chunks in %\n",
    "        self.desired_fs = config['desired_fs']\n",
    "        self.channel = config['channel']\n",
    "        self.log = config['log']\n",
    "        self.color = config['color']\n",
    "\n",
    "        self.nfft = config['nfft']\n",
    "        self.win_len = config['win_len']\n",
    "        self.hop_len = config['hop_len']\n",
    "        self.win_overlap = self.win_len - self.hop_len\n",
    "\n",
    "        # Get the color map by name:\n",
    "        #self.cmap = plt.get_cmap(config['cmap'])\n",
    "\n",
    "        # Path to dataset\n",
    "        self.path_to_dataset = pathlib.Path(path_to_dataset)\n",
    "        self.wavs_folder = self.path_to_dataset.joinpath('audio')\n",
    "        self.annotations_folder = self.path_to_dataset.joinpath('annotations')\n",
    "        self.images_folder = self.path_to_dataset.joinpath('images')\n",
    "        self.labels_folder = self.path_to_dataset.joinpath('labels')\n",
    "        if not self.images_folder.exists():\n",
    "            os.mkdir(self.images_folder)\n",
    "            os.mkdir(self.labels_folder)\n",
    "            os.mkdir(self.labels_folder.joinpath('backgrounds'))\n",
    "\n",
    "        self.F_MIN = 0\n",
    "        self.blocksize = int(self.duration * self.desired_fs)\n",
    "        self.config = config\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self.config.keys():\n",
    "            self.config[key] = value\n",
    "        self.__dict__[key] = value\n",
    "\n",
    "    def save_config(self, config_path):\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f)\n",
    "\n",
    "    def create_train_dataset(self, class_encoding):\n",
    "        indices_per_deployment = self.convert_challenge_annotations_to_yolo(class_encoding=class_encoding)\n",
    "        selected_samples = self.select_background_labels(indices_per_deployment)\n",
    "        self.create_spectrograms(selected_samples=selected_samples)\n",
    "\n",
    "    def create_test_dataset(self, class_encoding):\n",
    "        indices_per_deployment = self.convert_challenge_annotations_to_yolo(class_encoding=class_encoding)\n",
    "        selected_samples = self.select_all_background_labels(indices_per_deployment)\n",
    "        self.create_spectrograms(selected_samples=selected_samples)\n",
    "\n",
    "    def select_all_background_labels(self, indices_per_deployment):\n",
    "        for dataset, indices_dataset in indices_per_deployment.items():\n",
    "            background_indices = indices_dataset['background']\n",
    "            indices_per_deployment[dataset]['selected_background'] = background_indices\n",
    "\n",
    "            for selection in background_indices:\n",
    "                shutil.move(self.labels_folder.joinpath('backgrounds', selection + '.txt'),\n",
    "                            self.labels_folder.joinpath(selection + '.txt'))\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def select_background_labels(self, indices_per_deployment):\n",
    "        for dataset, indices_dataset in indices_per_deployment.items():\n",
    "            label_indices = indices_dataset['labels']\n",
    "            background_indices = indices_dataset['background']\n",
    "            n_labels = min(len(label_indices), len(background_indices))\n",
    "            print(f'There are {n_labels} labels in the dataset {dataset}')\n",
    "\n",
    "            selected_background = random.sample(background_indices, n_labels)\n",
    "            indices_per_deployment[dataset]['selected_background'] = selected_background\n",
    "\n",
    "            for selection in selected_background:\n",
    "                shutil.move(self.labels_folder.joinpath('backgrounds', selection + '.txt'),\n",
    "                            self.labels_folder.joinpath(selection + '.txt'))\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def create_spectrograms(self, selected_samples, overwrite=True):\n",
    "        # First, create all the images\n",
    "        print('Creating spectrograms...')\n",
    "        for dataset, indices_dataset in selected_samples.items():\n",
    "            selected_indices = indices_dataset['selected_background'] + indices_dataset['labels']\n",
    "            for sample_i in tqdm(selected_indices):\n",
    "                img_path = self.images_folder.joinpath(sample_i + '.png')\n",
    "                wav_name = '_'.join(sample_i.split('_')[1:3])\n",
    "                wav_path = self.wavs_folder.joinpath(dataset, wav_name + '.wav')\n",
    "                i = float(sample_i.split('_')[-1])\n",
    "                if overwrite or (not img_path.exists()):\n",
    "                    start_chunk = int(i * self.blocksize)\n",
    "                    chunk, fs = torchaudio.load(wav_path, normalize=True, frame_offset=start_chunk,\n",
    "                                                num_frames=self.blocksize)\n",
    "                    chunk = chunk[0, :]\n",
    "\n",
    "                    if len(chunk) < self.blocksize:\n",
    "                        chunk = F_general.pad(chunk, (0, self.blocksize - len(chunk)))\n",
    "                    img, f = self.create_chunk_spectrogram(chunk)\n",
    "\n",
    "                    if self.log:\n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.pcolormesh(img[:, :, ::-1])\n",
    "                        ax.set_yscale('symlog')\n",
    "                        plt.axis('off')\n",
    "                        plt.ylim(bottom=3)\n",
    "                        plt.savefig(img_path, bbox_inches='tight', pad_inches=0)\n",
    "                    else:\n",
    "                        Image.fromarray(np.flipud(img)).save(img_path)\n",
    "                    plt.close()\n",
    "                i += self.overlap\n",
    "\n",
    "    def create_chunk_spectrogram(self, chunk):\n",
    "        sos = scipy.signal.iirfilter(20, [5, 124], rp=None, rs=None, btype='band',\n",
    "                                     analog=False, ftype='butter', output='sos',\n",
    "                                     fs=self.desired_fs)\n",
    "        chunk = scipy.signal.sosfilt(sos, chunk)\n",
    "        f, t, sxx = scipy.signal.spectrogram(chunk, fs=self.desired_fs, window=('hann'),\n",
    "                                             nperseg=self.win_len,\n",
    "                                             noverlap=self.win_overlap, nfft=self.nfft,\n",
    "                                             detrend=False,\n",
    "                                             return_onesided=True, scaling='density', axis=-1,\n",
    "                                             mode='magnitude')\n",
    "        sxx = 1 - sxx\n",
    "        per = np.percentile(sxx.flatten(), 98)\n",
    "        sxx = (sxx - sxx.min()) / (per - sxx.min())\n",
    "        sxx[sxx > 1] = 1\n",
    "        img = np.array(sxx * 255, dtype=np.uint8)\n",
    "        return img, f\n",
    "\n",
    "    def convert_challenge_annotations_to_yolo(self, class_encoding=None):\n",
    "        \"\"\"\n",
    "        :param annotations_file:\n",
    "        :param labels_to_exclude: list\n",
    "        :param class_encoding: should be a dict with the name of the Tag as a key and an int as the value, for the\n",
    "        yolo classes\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        f_bandwidth = (self.desired_fs / 2) - self.F_MIN\n",
    "        indices_per_deployment = {}\n",
    "        for selections_path in list(self.annotations_folder.glob('*.csv')):\n",
    "            background_indices = []\n",
    "            labels_indices = []\n",
    "            selections = pd.read_csv(selections_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "            selections.loc[selections['low_frequency'] < self.F_MIN, 'low_frequency'] = self.F_MIN\n",
    "            selections['height'] = (selections['high_frequency'] - selections['low_frequency']) / f_bandwidth\n",
    "\n",
    "            # The y is from the TOP!\n",
    "            selections['y'] = 1 - (selections['high_frequency'] / f_bandwidth)\n",
    "\n",
    "            # Deal with datetime\n",
    "            selections['start_datetime_wav'] = pd.to_datetime(selections['filename'].apply(lambda y: y.split('.')[0]),\n",
    "                                                              format='%Y-%m-%dT%H-%M-%S_%f')\n",
    "            selections['start_datetime_wav'] = selections['start_datetime_wav'].dt.tz_localize('UTC')\n",
    "            selections['start_seconds'] = (selections.start_datetime - selections.start_datetime_wav).dt.total_seconds()\n",
    "            selections['end_seconds'] = (selections.end_datetime - selections.start_datetime_wav).dt.total_seconds()\n",
    "\n",
    "            pbar = tqdm(total=len(selections['filename'].unique()))\n",
    "\n",
    "            dataset_name = selections.iloc[0].dataset\n",
    "            for wav_name, wav_selections in selections.groupby('filename'):\n",
    "                wav_file_path = self.wavs_folder.joinpath(dataset_name, wav_name)\n",
    "                waveform_info = torchaudio.info(wav_file_path)\n",
    "\n",
    "                i = 0.0\n",
    "                while (i * self.duration + self.duration / 2) < (waveform_info.num_frames / waveform_info.sample_rate):\n",
    "                    start_seconds = i * self.duration\n",
    "                    end_seconds = start_seconds + self.duration\n",
    "\n",
    "                    start_mask = (wav_selections['start_seconds'] >= start_seconds) & (wav_selections[\n",
    "                                                                                           'start_seconds'] <= end_seconds)\n",
    "                    end_mask = (wav_selections['start_seconds'] >= start_seconds) & (wav_selections[\n",
    "                                                                                         'end_seconds'] <= end_seconds)\n",
    "                    chunk_selection = wav_selections.loc[start_mask | end_mask]\n",
    "                    chunk_selection = chunk_selection.assign(start_x=((chunk_selection['start_seconds'] - i * self.duration) / self.duration).clip(lower=0, upper=1).values)\n",
    "                    chunk_selection = chunk_selection.assign(end_x=((chunk_selection['end_seconds'] - i * self.duration) / self.duration).clip(lower=0, upper=1).values)\n",
    "\n",
    "                    # compute the width in pixels\n",
    "                    chunk_selection = chunk_selection.assign(width=(chunk_selection['end_x'] - chunk_selection['start_x']).values)\n",
    "\n",
    "                    # Save the chunk detections so that they are with the yolo format\n",
    "                    # <class > < x > < y > < width > < height >\n",
    "                    chunk_selection = chunk_selection.assign(x=(chunk_selection['start_x'] + chunk_selection['width'] / 2).values)\n",
    "                    chunk_selection.loc[:, 'y'] = (chunk_selection['y'] + chunk_selection['height'] / 2).values\n",
    "\n",
    "                    # if ((chunk_selection.x + chunk_selection.width/2) > 1).sum() > 0 or (chunk_selection.y > 1).sum() > 0:\n",
    "                    #     print(chunk_selection)\n",
    "                    #     print(start_seconds, end_seconds)\n",
    "                    chunk_selection = chunk_selection.replace(to_replace=class_encoding).infer_objects(copy=False)\n",
    "                    new_name = dataset_name + '_' + wav_name.replace('.wav', '_%s' % i)\n",
    "\n",
    "                    if len(chunk_selection) > 0:\n",
    "                        labels_indices.append(new_name)\n",
    "                        label_path = self.labels_folder.joinpath(new_name + '.txt')\n",
    "                    else:\n",
    "                        background_indices.append(new_name)\n",
    "                        label_path = self.labels_folder.joinpath('backgrounds', new_name + '.txt')\n",
    "\n",
    "                    chunk_selection[[\n",
    "                        'annotation',\n",
    "                        'x',\n",
    "                        'y',\n",
    "                        'width',\n",
    "                        'height']].to_csv(label_path, header=None, index=None, sep=' ', mode='w')\n",
    "                    # Add the station if the image adds it as well!\n",
    "                    i += self.overlap\n",
    "                pbar.update(1)\n",
    "            indices_per_deployment[dataset_name] = {'background': background_indices, 'labels': labels_indices}\n",
    "            pbar.close()\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def convert_yolo_detections_to_csv(self, predictions_folder, reverse_class_encoding):\n",
    "        # Convert to DataFrame\n",
    "        labels_folder = predictions_folder.joinpath('labels')\n",
    "\n",
    "        columns = ['dataset', 'filename', 'annotation', 'low_frequency', 'high_frequency',\n",
    "                   'start_datetime', 'end_datetime', 'confidence', 'offset_i']\n",
    "\n",
    "        detections_list = []\n",
    "        f_bandwidth = (self.desired_fs / 2) - self.F_MIN\n",
    "        for txt_label in tqdm(labels_folder.glob('*.txt'), total=len(list(labels_folder.glob('*.txt')))):\n",
    "            name_parts = txt_label.name.split('_')\n",
    "            wav_name = '_'.join(name_parts[1:-1]) + '.wav'\n",
    "            dataset_name = name_parts[0]\n",
    "\n",
    "            offset_i = float(name_parts[-1].split('.txt')[0])\n",
    "            detections_i = pd.read_table(txt_label, header=None, sep=' ', names=['annotation', 'x', 'y',\n",
    "                                                                                 'width', 'height', 'confidence'])\n",
    "            detections_i['filename'] = wav_name\n",
    "            detections_i['dataset'] = dataset_name\n",
    "            detections_i['offset_i'] = offset_i\n",
    "            detections_list.append(detections_i)\n",
    "\n",
    "        detections = pd.concat(detections_list, ignore_index=True)\n",
    "\n",
    "        # start and end in seconds from beginning of file\n",
    "        detections['start_seconds'] = (detections.x - detections.width / 2 + detections.offset_i) * self.duration\n",
    "        detections['end_seconds'] = detections.width * self.duration + detections['start_seconds']\n",
    "\n",
    "        # start of wav file datetime\n",
    "        detections['start_datetime_wav'] = pd.to_datetime(detections['filename'].apply(lambda y: y.split('.')[0]),\n",
    "                                                          format='%Y-%m-%dT%H-%M-%S_%f')\n",
    "        detections['start_datetime_wav'] = detections['start_datetime_wav'].dt.tz_localize('UTC')\n",
    "\n",
    "        # Compute absolute start and end time\n",
    "        detections['start_datetime'] = detections['start_datetime_wav'] + pd.to_timedelta(detections['start_seconds'],\n",
    "                                                                                          unit='s')\n",
    "        detections['end_datetime'] = detections['start_datetime_wav'] + pd.to_timedelta(detections['end_seconds'],\n",
    "                                                                                        unit='s')\n",
    "\n",
    "        detections['start_datetime'] = detections['start_datetime'].apply(lambda x: x.isoformat(timespec='microseconds'))\n",
    "        detections['end_datetime'] = detections['end_datetime'].apply(lambda x: x.isoformat(timespec='microseconds'))\n",
    "\n",
    "        # Frequency boundaries\n",
    "        detections['low_frequency'] = ((1 - (detections.y + detections.height / 2)) * f_bandwidth).clip(lower=0)\n",
    "        detections['high_frequency'] = ((1 - (detections.y - detections.height / 2)) * f_bandwidth).clip(upper=self.desired_fs/2)\n",
    "\n",
    "        # Change the annotation names\n",
    "        detections['annotation'] = detections['annotation'].replace(to_replace=reverse_class_encoding).infer_objects(copy=False)\n",
    "        detections[columns].to_csv(self.path_to_dataset.joinpath('predictions.csv'), index=False)\n",
    "\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93c25a3-0a25-4be8-b002-787f440fb284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Where is the dataset folder? isibhv/projects/p_OZA_AI/biodcase_train_vali/train\n",
      "Is it for the training dataset y/n? y\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'isibhv/projects/p_OZA_AI/biodcase_train_vali/train/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m f = \u001b[38;5;28mopen\u001b[39m(config_path)\n\u001b[32m      7\u001b[39m config = json.load(f)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m ds = \u001b[43mYOLODataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_mode:\n\u001b[32m     11\u001b[39m     ds.create_train_dataset(class_encoding=config[\u001b[33m'\u001b[39m\u001b[33mclass_encoding\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mYOLODataset.__init__\u001b[39m\u001b[34m(self, config, path_to_dataset)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.labels_folder = \u001b[38;5;28mself\u001b[39m.path_to_dataset.joinpath(\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.images_folder.exists():\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimages_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m.labels_folder)\n\u001b[32m     28\u001b[39m     os.mkdir(\u001b[38;5;28mself\u001b[39m.labels_folder.joinpath(\u001b[33m'\u001b[39m\u001b[33mbackgrounds\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'isibhv/projects/p_OZA_AI/biodcase_train_vali/train/images'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    path_to_dataset = input('Where is the dataset folder?')\n",
    "\n",
    "    train_mode = input('Is it for the training dataset y/n?') == 'y'\n",
    "    config_path = './dataset_config.json'\n",
    "    f = open(config_path)\n",
    "    config = json.load(f)\n",
    "\n",
    "    ds = YOLODataset(config, path_to_dataset)\n",
    "    if train_mode:\n",
    "        ds.create_train_dataset(class_encoding=config['class_encoding'])\n",
    "    else:\n",
    "        ds.create_test_dataset(class_encoding=config['class_encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab55dd4-42a1-4a8e-8d64-b7e603c1e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset:\n",
    "    def __init__(self, config, path_to_dataset):\n",
    "        # Spectrogram settings\n",
    "        self.duration = config['duration']\n",
    "        self.overlap = config['overlap']  # overlap of the chunks in %\n",
    "        self.desired_fs = config['desired_fs']\n",
    "        self.channel = config['channel']\n",
    "        self.log = config['log']\n",
    "        self.color = config['color']\n",
    "\n",
    "        self.nfft = config['nfft']\n",
    "        self.win_len = config['win_len']\n",
    "        self.hop_len = config['hop_len']\n",
    "        self.win_overlap = self.win_len - self.hop_len\n",
    "\n",
    "        # Get the color map by name:\n",
    "        #self.cmap = plt.get_cmap(config['cmap'])\n",
    "\n",
    "        # Path to dataset\n",
    "        self.path_to_dataset = pathlib.Path(path_to_dataset)\n",
    "        self.wavs_folder = self.path_to_dataset.joinpath('audio')\n",
    "        self.annotations_folder = self.path_to_dataset.joinpath('annotations')\n",
    "        self.images_folder = self.pathlib.Path('albedo/work/projects/p_OZA_AI/train_test/images')\n",
    "        self.labels_folder = self.pathlib.Path('albedo/work/projects/p_OZA_AI/train_test/labels')\n",
    "        if not self.images_folder.exists():\n",
    "            os.mkdir(self.images_folder)\n",
    "            os.mkdir(self.labels_folder)\n",
    "            os.mkdir(self.labels_folder.joinpath('backgrounds'))\n",
    "\n",
    "        self.F_MIN = 0\n",
    "        self.blocksize = int(self.duration * self.desired_fs)\n",
    "        self.config = config\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self.config.keys():\n",
    "            self.config[key] = value\n",
    "        self.__dict__[key] = value\n",
    "\n",
    "    def save_config(self, config_path):\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f)\n",
    "\n",
    "    def create_train_dataset(self, class_encoding):\n",
    "        indices_per_deployment = self.convert_challenge_annotations_to_yolo(class_encoding=class_encoding)\n",
    "        selected_samples = self.select_background_labels(indices_per_deployment)\n",
    "        self.create_spectrograms(selected_samples=selected_samples)\n",
    "\n",
    "    def create_test_dataset(self, class_encoding):\n",
    "        indices_per_deployment = self.convert_challenge_annotations_to_yolo(class_encoding=class_encoding)\n",
    "        selected_samples = self.select_all_background_labels(indices_per_deployment)\n",
    "        self.create_spectrograms(selected_samples=selected_samples)\n",
    "\n",
    "    def select_all_background_labels(self, indices_per_deployment):\n",
    "        for dataset, indices_dataset in indices_per_deployment.items():\n",
    "            background_indices = indices_dataset['background']\n",
    "            indices_per_deployment[dataset]['selected_background'] = background_indices\n",
    "\n",
    "            for selection in background_indices:\n",
    "                shutil.move(self.labels_folder.joinpath('backgrounds', selection + '.txt'),\n",
    "                            self.labels_folder.joinpath(selection + '.txt'))\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def select_background_labels(self, indices_per_deployment):\n",
    "        for dataset, indices_dataset in indices_per_deployment.items():\n",
    "            label_indices = indices_dataset['labels']\n",
    "            background_indices = indices_dataset['background']\n",
    "            n_labels = min(len(label_indices), len(background_indices))\n",
    "            print(f'There are {n_labels} labels in the dataset {dataset}')\n",
    "\n",
    "            selected_background = random.sample(background_indices, n_labels)\n",
    "            indices_per_deployment[dataset]['selected_background'] = selected_background\n",
    "\n",
    "            for selection in selected_background:\n",
    "                shutil.move(self.labels_folder.joinpath('backgrounds', selection + '.txt'),\n",
    "                            self.labels_folder.joinpath(selection + '.txt'))\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def create_spectrograms(self, selected_samples, overwrite=True):\n",
    "        # First, create all the images\n",
    "        print('Creating spectrograms...')\n",
    "        for dataset, indices_dataset in selected_samples.items():\n",
    "            selected_indices = indices_dataset['selected_background'] + indices_dataset['labels']\n",
    "            for sample_i in tqdm(selected_indices):\n",
    "                img_path = self.images_folder.joinpath(sample_i + '.png')\n",
    "                wav_name = '_'.join(sample_i.split('_')[1:3])\n",
    "                wav_path = self.wavs_folder.joinpath(dataset, wav_name + '.wav')\n",
    "                i = float(sample_i.split('_')[-1])\n",
    "                if overwrite or (not img_path.exists()):\n",
    "                    start_chunk = int(i * self.blocksize)\n",
    "                    chunk, fs = torchaudio.load(wav_path, normalize=True, frame_offset=start_chunk,\n",
    "                                                num_frames=self.blocksize)\n",
    "                    chunk = chunk[0, :]\n",
    "\n",
    "                    if len(chunk) < self.blocksize:\n",
    "                        chunk = F_general.pad(chunk, (0, self.blocksize - len(chunk)))\n",
    "                    img, f = self.create_chunk_spectrogram(chunk)\n",
    "\n",
    "                    if self.log:\n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.pcolormesh(img[:, :, ::-1])\n",
    "                        ax.set_yscale('symlog')\n",
    "                        plt.axis('off')\n",
    "                        plt.ylim(bottom=3)\n",
    "                        plt.savefig(img_path, bbox_inches='tight', pad_inches=0)\n",
    "                    else:\n",
    "                        Image.fromarray(np.flipud(img)).save(img_path)\n",
    "                    plt.close()\n",
    "                i += self.overlap\n",
    "\n",
    "    def create_chunk_spectrogram(self, chunk):\n",
    "        sos = scipy.signal.iirfilter(20, [5, 124], rp=None, rs=None, btype='band',\n",
    "                                     analog=False, ftype='butter', output='sos',\n",
    "                                     fs=self.desired_fs)\n",
    "        chunk = scipy.signal.sosfilt(sos, chunk)\n",
    "        f, t, sxx = scipy.signal.spectrogram(chunk, fs=self.desired_fs, window=('hann'),\n",
    "                                             nperseg=self.win_len,\n",
    "                                             noverlap=self.win_overlap, nfft=self.nfft,\n",
    "                                             detrend=False,\n",
    "                                             return_onesided=True, scaling='density', axis=-1,\n",
    "                                             mode='magnitude')\n",
    "        sxx = 1 - sxx\n",
    "        per = np.percentile(sxx.flatten(), 98)\n",
    "        sxx = (sxx - sxx.min()) / (per - sxx.min())\n",
    "        sxx[sxx > 1] = 1\n",
    "        img = np.array(sxx * 255, dtype=np.uint8)\n",
    "        return img, f\n",
    "\n",
    "    def convert_challenge_annotations_to_yolo(self, class_encoding=None):\n",
    "        \"\"\"\n",
    "        :param annotations_file:\n",
    "        :param labels_to_exclude: list\n",
    "        :param class_encoding: should be a dict with the name of the Tag as a key and an int as the value, for the\n",
    "        yolo classes\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        f_bandwidth = (self.desired_fs / 2) - self.F_MIN\n",
    "        indices_per_deployment = {}\n",
    "        for selections_path in list(self.annotations_folder.glob('*.csv')):\n",
    "            background_indices = []\n",
    "            labels_indices = []\n",
    "            selections = pd.read_csv(selections_path, parse_dates=['start_datetime', 'end_datetime'])\n",
    "            selections.loc[selections['low_frequency'] < self.F_MIN, 'low_frequency'] = self.F_MIN\n",
    "            selections['height'] = (selections['high_frequency'] - selections['low_frequency']) / f_bandwidth\n",
    "\n",
    "            # The y is from the TOP!\n",
    "            selections['y'] = 1 - (selections['high_frequency'] / f_bandwidth)\n",
    "\n",
    "            # Deal with datetime\n",
    "            selections['start_datetime_wav'] = pd.to_datetime(selections['filename'].apply(lambda y: y.split('.')[0]),\n",
    "                                                              format='%Y-%m-%dT%H-%M-%S_%f')\n",
    "            selections['start_datetime_wav'] = selections['start_datetime_wav'].dt.tz_localize('UTC')\n",
    "            selections['start_seconds'] = (selections.start_datetime - selections.start_datetime_wav).dt.total_seconds()\n",
    "            selections['end_seconds'] = (selections.end_datetime - selections.start_datetime_wav).dt.total_seconds()\n",
    "\n",
    "            pbar = tqdm(total=len(selections['filename'].unique()))\n",
    "\n",
    "            dataset_name = selections.iloc[0].dataset\n",
    "            for wav_name, wav_selections in selections.groupby('filename'):\n",
    "                wav_file_path = self.wavs_folder.joinpath(dataset_name, wav_name)\n",
    "                waveform_info = torchaudio.info(wav_file_path)\n",
    "\n",
    "                i = 0.0\n",
    "                while (i * self.duration + self.duration / 2) < (waveform_info.num_frames / waveform_info.sample_rate):\n",
    "                    start_seconds = i * self.duration\n",
    "                    end_seconds = start_seconds + self.duration\n",
    "\n",
    "                    start_mask = (wav_selections['start_seconds'] >= start_seconds) & (wav_selections[\n",
    "                                                                                           'start_seconds'] <= end_seconds)\n",
    "                    end_mask = (wav_selections['start_seconds'] >= start_seconds) & (wav_selections[\n",
    "                                                                                         'end_seconds'] <= end_seconds)\n",
    "                    chunk_selection = wav_selections.loc[start_mask | end_mask]\n",
    "                    chunk_selection = chunk_selection.assign(start_x=((chunk_selection['start_seconds'] - i * self.duration) / self.duration).clip(lower=0, upper=1).values)\n",
    "                    chunk_selection = chunk_selection.assign(end_x=((chunk_selection['end_seconds'] - i * self.duration) / self.duration).clip(lower=0, upper=1).values)\n",
    "\n",
    "                    # compute the width in pixels\n",
    "                    chunk_selection = chunk_selection.assign(width=(chunk_selection['end_x'] - chunk_selection['start_x']).values)\n",
    "\n",
    "                    # Save the chunk detections so that they are with the yolo format\n",
    "                    # <class > < x > < y > < width > < height >\n",
    "                    chunk_selection = chunk_selection.assign(x=(chunk_selection['start_x'] + chunk_selection['width'] / 2).values)\n",
    "                    chunk_selection.loc[:, 'y'] = (chunk_selection['y'] + chunk_selection['height'] / 2).values\n",
    "\n",
    "                    # if ((chunk_selection.x + chunk_selection.width/2) > 1).sum() > 0 or (chunk_selection.y > 1).sum() > 0:\n",
    "                    #     print(chunk_selection)\n",
    "                    #     print(start_seconds, end_seconds)\n",
    "                    chunk_selection = chunk_selection.replace(to_replace=class_encoding).infer_objects(copy=False)\n",
    "                    new_name = dataset_name + '_' + wav_name.replace('.wav', '_%s' % i)\n",
    "\n",
    "                    if len(chunk_selection) > 0:\n",
    "                        labels_indices.append(new_name)\n",
    "                        label_path = self.labels_folder.joinpath(new_name + '.txt')\n",
    "                    else:\n",
    "                        background_indices.append(new_name)\n",
    "                        label_path = self.labels_folder.joinpath('backgrounds', new_name + '.txt')\n",
    "\n",
    "                    chunk_selection[[\n",
    "                        'annotation',\n",
    "                        'x',\n",
    "                        'y',\n",
    "                        'width',\n",
    "                        'height']].to_csv(label_path, header=None, index=None, sep=' ', mode='w')\n",
    "                    # Add the station if the image adds it as well!\n",
    "                    i += self.overlap\n",
    "                pbar.update(1)\n",
    "            indices_per_deployment[dataset_name] = {'background': background_indices, 'labels': labels_indices}\n",
    "            pbar.close()\n",
    "\n",
    "        return indices_per_deployment\n",
    "\n",
    "    def convert_yolo_detections_to_csv(self, predictions_folder, reverse_class_encoding):\n",
    "        # Convert to DataFrame\n",
    "        labels_folder = predictions_folder.joinpath('labels')\n",
    "\n",
    "        columns = ['dataset', 'filename', 'annotation', 'low_frequency', 'high_frequency',\n",
    "                   'start_datetime', 'end_datetime', 'confidence', 'offset_i']\n",
    "\n",
    "        detections_list = []\n",
    "        f_bandwidth = (self.desired_fs / 2) - self.F_MIN\n",
    "        for txt_label in tqdm(labels_folder.glob('*.txt'), total=len(list(labels_folder.glob('*.txt')))):\n",
    "            name_parts = txt_label.name.split('_')\n",
    "            wav_name = '_'.join(name_parts[1:-1]) + '.wav'\n",
    "            dataset_name = name_parts[0]\n",
    "\n",
    "            offset_i = float(name_parts[-1].split('.txt')[0])\n",
    "            detections_i = pd.read_table(txt_label, header=None, sep=' ', names=['annotation', 'x', 'y',\n",
    "                                                                                 'width', 'height', 'confidence'])\n",
    "            detections_i['filename'] = wav_name\n",
    "            detections_i['dataset'] = dataset_name\n",
    "            detections_i['offset_i'] = offset_i\n",
    "            detections_list.append(detections_i)\n",
    "\n",
    "        detections = pd.concat(detections_list, ignore_index=True)\n",
    "\n",
    "        # start and end in seconds from beginning of file\n",
    "        detections['start_seconds'] = (detections.x - detections.width / 2 + detections.offset_i) * self.duration\n",
    "        detections['end_seconds'] = detections.width * self.duration + detections['start_seconds']\n",
    "\n",
    "        # start of wav file datetime\n",
    "        detections['start_datetime_wav'] = pd.to_datetime(detections['filename'].apply(lambda y: y.split('.')[0]),\n",
    "                                                          format='%Y-%m-%dT%H-%M-%S_%f')\n",
    "        detections['start_datetime_wav'] = detections['start_datetime_wav'].dt.tz_localize('UTC')\n",
    "\n",
    "        # Compute absolute start and end time\n",
    "        detections['start_datetime'] = detections['start_datetime_wav'] + pd.to_timedelta(detections['start_seconds'],\n",
    "                                                                                          unit='s')\n",
    "        detections['end_datetime'] = detections['start_datetime_wav'] + pd.to_timedelta(detections['end_seconds'],\n",
    "                                                                                        unit='s')\n",
    "\n",
    "        detections['start_datetime'] = detections['start_datetime'].apply(lambda x: x.isoformat(timespec='microseconds'))\n",
    "        detections['end_datetime'] = detections['end_datetime'].apply(lambda x: x.isoformat(timespec='microseconds'))\n",
    "\n",
    "        # Frequency boundaries\n",
    "        detections['low_frequency'] = ((1 - (detections.y + detections.height / 2)) * f_bandwidth).clip(lower=0)\n",
    "        detections['high_frequency'] = ((1 - (detections.y - detections.height / 2)) * f_bandwidth).clip(upper=self.desired_fs/2)\n",
    "\n",
    "        # Change the annotation names\n",
    "        detections['annotation'] = detections['annotation'].replace(to_replace=reverse_class_encoding).infer_objects(copy=False)\n",
    "        detections[columns].to_csv(self.path_to_dataset.joinpath('predictions.csv'), index=False)\n",
    "\n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2883f5-2aef-49a4-a379-2c45da6f9a02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_to_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m f = \u001b[38;5;28mopen\u001b[39m(config_path)\n\u001b[32m      8\u001b[39m config = json.load(f)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ds = YOLODataset(config, \u001b[43mpath_to_dataset\u001b[49m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#if train_mode:\u001b[39;00m\n\u001b[32m     12\u001b[39m ds.create_train_dataset(class_encoding=config[\u001b[33m'\u001b[39m\u001b[33mclass_encoding\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'path_to_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #path_to_dataset = input('Where is the dataset folder?')\n",
    "    path_to_datset = 'isibhv/projects/p_OZA_AI/biodcase_train_vali/train_test'\n",
    "\n",
    "    #train_mode = input('Is it for the training dataset y/n?') == 'y'\n",
    "    config_path = './dataset_config.json'\n",
    "    f = open(config_path)\n",
    "    config = json.load(f)\n",
    "\n",
    "    ds = YOLODataset(config, path_to_dataset)\n",
    "    #if train_mode:\n",
    "    ds.create_train_dataset(class_encoding=config['class_encoding'])\n",
    "    #else:\n",
    "    #    ds.create_test_dataset(class_encoding=config['class_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4308fe7-5edc-4ae5-ae95-9f4c5b32cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn.functional as F_general\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53dc35f-80be-4726-b325-799af0e20d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e0b67d-4a4e-4a2c-b5d1-4aff963c2a56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'YOLODataset' object has no attribute 'pathlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m f = \u001b[38;5;28mopen\u001b[39m(config_path)\n\u001b[32m      8\u001b[39m config = json.load(f)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m ds = \u001b[43mYOLODataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#if train_mode:\u001b[39;00m\n\u001b[32m     12\u001b[39m ds.create_train_dataset(class_encoding=config[\u001b[33m'\u001b[39m\u001b[33mclass_encoding\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mYOLODataset.__init__\u001b[39m\u001b[34m(self, config, path_to_dataset)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.wavs_folder = \u001b[38;5;28mself\u001b[39m.path_to_dataset.joinpath(\u001b[33m'\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.annotations_folder = \u001b[38;5;28mself\u001b[39m.path_to_dataset.joinpath(\u001b[33m'\u001b[39m\u001b[33mannotations\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.images_folder = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpathlib\u001b[49m.Path(\u001b[33m'\u001b[39m\u001b[33malbedo/work/projects/p_OZA_AI/train_test/images\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.labels_folder = \u001b[38;5;28mself\u001b[39m.pathlib.Path(\u001b[33m'\u001b[39m\u001b[33malbedo/work/projects/p_OZA_AI/train_test/labels\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.images_folder.exists():\n",
      "\u001b[31mAttributeError\u001b[39m: 'YOLODataset' object has no attribute 'pathlib'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #path_to_dataset = input('Where is the dataset folder?')\n",
    "    path_to_dataset = 'isibhv/projects/p_OZA_AI/biodcase_train_vali/train_test'\n",
    "\n",
    "    #train_mode = input('Is it for the training dataset y/n?') == 'y'\n",
    "    config_path = './dataset_config.json'\n",
    "    f = open(config_path)\n",
    "    config = json.load(f)\n",
    "\n",
    "    ds = YOLODataset(config, path_to_dataset)\n",
    "    #if train_mode:\n",
    "    ds.create_train_dataset(class_encoding=config['class_encoding'])\n",
    "    #else:\n",
    "    #    ds.create_test_dataset(class_encoding=config['class_encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3ab13-7dfd-40f9-aae9-850d495d6607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLO_OZA",
   "language": "python",
   "name": "yolo_oza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
